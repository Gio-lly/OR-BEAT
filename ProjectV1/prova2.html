<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Onset Detection Web App</title>
    <style>
        #waveform {
            border: 1px solid black;
            width: 100%;
            height: 300px;
            margin-top: 20px;
        }

        button {
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <h1>Onset Detection</h1>
    <input type="file" id="fileInput" accept=".wav, .mp3">
    <button onclick="processAudio()">Process Audio</button>
    <button onclick="playAudio()" id="playButton" disabled>Play</button>
    <button onclick="stopAudio()" id="stopButton" disabled>Stop</button>
    <p id="status">Please upload a .wav or .mp3 file and click "Process Audio" to start.</p>

    <canvas id="waveform"></canvas>

    <script>
        let audioBuffer = null;
        let audioContext = null;
        let sourceNode = null;
        let onsetTimes = [];
        let animationFrameId = null;
        let currentTime = 0;

        let isDragging = false;
        let dragIndex = -1;

        let canvas = document.getElementById('waveform');
        let ctx = canvas.getContext('2d');

        // Processing the audio file
        async function processAudio() {
            const fileInput = document.getElementById('fileInput');
            const status = document.getElementById('status');
            
            if (!fileInput.files[0]) {
                status.textContent = 'Please upload a .wav or .mp3 file first.';
                return;
            }

            const file = fileInput.files[0];
            audioContext = new (window.AudioContext || window.webkitAudioContext)();

            status.textContent = 'Processing...';

            try {
                const arrayBuffer = await file.arrayBuffer();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                console.log("Audio decoded successfully.");

                onsetTimes = await getOnsetTimes(audioBuffer, audioContext);
                console.log("Detected onset times:", onsetTimes);

                if (onsetTimes.length === 0) {
                    status.textContent = 'No onsets detected.';
                    return;
                }

                const onsetTimesText = onsetTimes.map(time => time.toFixed(4)).join('\n');
                const blob = new Blob([onsetTimesText], { type: 'text/plain' });
                const link = document.createElement('a');
                link.href = URL.createObjectURL(blob);
                link.download = file.name.replace(/\.[^/.]+$/, '') + '.beatmap.txt';
                link.click();

                status.textContent = 'Onset detection complete! Downloading the .txt file...';

                // Draw waveform and onset markers
                drawWaveform(audioBuffer, canvas, ctx, onsetTimes);

                // Enable play and stop buttons after processing
                document.getElementById('playButton').disabled = false;
                document.getElementById('stopButton').disabled = false;

            } catch (error) {
                status.textContent = 'Error processing audio file.';
                console.error(error);
            }
        }

        // Function to get the onset times from the audio data
        async function getOnsetTimes(audioBuffer, audioContext) {
            const channelData = audioBuffer.getChannelData(0); // Get channel data
            const sampleRate = audioBuffer.sampleRate;
            const onsetTimes = [];
            const windowSize = 1024; // Number of samples per window
            const hopSize = windowSize / 4; // Shift by 25% per step

            const amplitudeThreshold = 0.0001; // Small threshold to avoid considering zero amplitude as onset

            let lastAmplitude = 0;
            const duration = audioBuffer.duration; // Duration of the audio in seconds

            for (let i = 0; i < channelData.length - windowSize; i += hopSize) {
                const windowData = channelData.slice(i, i + windowSize);
                const amplitude = getAmplitude(windowData);
                console.log(`Window ${i} - Amplitude: ${amplitude}`); // Debugging log

                // Only consider this window as an onset if amplitude is above the threshold and there's significant change
                if (amplitude > amplitudeThreshold && Math.abs(amplitude - lastAmplitude) > 0.1) {
                    const onsetTime = i / sampleRate;
                    if (onsetTime < duration) {
                        onsetTimes.push(onsetTime);
                    }
                }

                lastAmplitude = amplitude;
            }

            return onsetTimes;
        }

        function getAmplitude(data) {
            let sum = 0;
            for (let i = 0; i < data.length; i++) {
                sum += Math.abs(data[i]);
            }
            return sum / data.length; // Average absolute amplitude
        }

        // Draw waveform and onset markers
        function drawWaveform(audioBuffer, canvas, ctx, onsetTimes) {
            const channelData = audioBuffer.getChannelData(0); // Use first channel data
            const sampleRate = audioBuffer.sampleRate;
            const duration = audioBuffer.duration;

            const canvasWidth = canvas.width = 1000; // Set canvas width
            const canvasHeight = canvas.height = 300; // Set canvas height
            const numSamples = Math.floor(duration * sampleRate);
            const pixelsPerSample = canvasWidth / numSamples;

            ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas

            // Draw the waveform
            ctx.beginPath();
            ctx.moveTo(0, canvasHeight / 2); // Start in the middle of the canvas

            for (let i = 0; i < numSamples; i++) {
                const sample = channelData[i];
                const y = (sample * canvasHeight / 2) + (canvasHeight / 2); // Scale the sample
                ctx.lineTo(i * pixelsPerSample, y);
            }

            ctx.strokeStyle = 'black';
            ctx.lineWidth = 1;
            ctx.stroke();

            // Draw onset markers (red vertical lines)
            ctx.strokeStyle = 'red';
            ctx.lineWidth = 2;

            onsetTimes.forEach((onsetTime, index) => {
                const xPos = onsetTime * sampleRate * pixelsPerSample;
                ctx.beginPath();
                ctx.moveTo(xPos, 0);
                ctx.lineTo(xPos, canvasHeight);
                ctx.stroke();

                // Add the index as an attribute to each line for dragging
                onsetTimes[index].xPos = xPos;
            });

            // Draw the blue line (to track playback position)
            ctx.strokeStyle = 'blue';
            ctx.lineWidth = 2;
            const currentXPos = currentTime * sampleRate * pixelsPerSample;
            ctx.beginPath();
            ctx.moveTo(currentXPos, 0);
            ctx.lineTo(currentXPos, canvasHeight);
            ctx.stroke();
        }

        // Event listener for mouse down (to start dragging)
        canvas.addEventListener('mousedown', function (e) {
            const mouseX = e.offsetX;
            const sampleRate = audioBuffer.sampleRate;
            const pixelsPerSample = canvas.width / (audioBuffer.duration * sampleRate);

            // Check if mouse click is close to any red line (onset marker)
            for (let i = 0; i < onsetTimes.length; i++) {
                const onsetX = onsetTimes[i].xPos;
                if (Math.abs(mouseX - onsetX) < 10) { // Click near the line
                    isDragging = true;
                    dragIndex = i;
                    break;
                }
            }
        });

        // Event listener for mouse move (to drag the lines)
        canvas.addEventListener('mousemove', function (e) {
            if (isDragging && dragIndex !== -1) {
                const mouseX = e.offsetX;
                const sampleRate = audioBuffer.sampleRate;
                const pixelsPerSample = canvas.width / (audioBuffer.duration * sampleRate);

                // Update the onset time for the dragged line
                const newTime = (mouseX / pixelsPerSample) / sampleRate;
                onsetTimes[dragIndex] = newTime;

                // Redraw the waveform with updated marker positions
                drawWaveform(audioBuffer, canvas, ctx, onsetTimes);
            }
        });

        // Event listener for mouse up (to stop dragging)
        canvas.addEventListener('mouseup', function () {
            isDragging = false;
            dragIndex = -1;
        });

        // Play the audio
        function playAudio() {
            if (!audioBuffer || !audioContext) return;

            // Create a new source node to play the audio
            sourceNode = audioContext.createBufferSource();
            sourceNode.buffer = audioBuffer;
            sourceNode.connect(audioContext.destination);

            sourceNode.start(0);
            console.log("Audio is playing...");

            // Disable the play button during playback
            document.getElementById('playButton').disabled = true;
            document.getElementById('stopButton').disabled = false;

            // Start animating the blue line
            animationFrameId = requestAnimationFrame(animatePlayback);

            sourceNode.onended = function () {
                // Re-enable play button after audio finishes playing
                document.getElementById('playButton').disabled = false;
                document.getElementById('stopButton').disabled = true;

                cancelAnimationFrame(animationFrameId); // Stop animation when audio ends
            };
        }

        // Stop the audio
        function stopAudio() {
            if (sourceNode) {
                sourceNode.stop();
                console.log("Audio stopped.");

                // Re-enable the play button
                document.getElementById('playButton').disabled = false;
                document.getElementById('stopButton').disabled = true;

                cancelAnimationFrame(animationFrameId); // Stop the animation
            }
        }

        // Animation to track audio playback
        function animatePlayback() {
            if (!audioContext) return;

            // Update currentTime to reflect the audio playback position
            currentTime = audioContext.currentTime;

            // Redraw waveform with moving blue line
            drawWaveform(audioBuffer, canvas, ctx, onsetTimes);

            // Continue animation as long as audio is playing
            if (currentTime < audioBuffer.duration) {
                animationFrameId = requestAnimationFrame(animatePlayback);
            }
        }
    </script>
</body>
</html>
